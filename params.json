{"name":"Predmachlearn","tagline":"","body":"---\r\ntitle: \"Predicting the quality of dumbbell curl exercises\"\r\nauthor: \"Eusebio Rufian-Zilbermann\"\r\noutput: html_document\r\ngeometry: top=0.5in, bottom=1in\r\n---\r\n```{r Code, echo=FALSE, results='hide', warning=FALSE, message=FALSE, cache=TRUE}\r\nrequire(doParallel)\r\nrequire(nnet)\r\nrequire(caret)\r\nrequire(dplyr)\r\nrequire(ggplot2)\r\nrequire(randomForest)\r\nrequire(e1071)\r\ncl <- makeCluster(detectCores())\r\nregisterDoParallel(cl)\r\nset.seed(13234)\r\npml<-read.csv2(\"pml-training.csv\", header=TRUE, sep = \",\", dec=\".\", na.strings=\"NA\")\r\npartitionFactor<-transmute(pml,paste(toString(user_name[1]),toString(classe[1]),sep=\"\"))\r\ninTrain = createDataPartition(partitionFactor[,1],p=0.6,list=FALSE)\r\n# Apologies for the ugly use of copy..paste programming\r\n# Some refactoring would be appropriate (if I have the time to do it...)\r\npml16f = pml[grepl(\"^classe$|^roll_|^yaw_|^pitch_|^total_accel_\",names(pml))]\r\ntraining16f = pml16f[ inTrain,]\r\ntesting16f = pml16f[-inTrain,]\r\npml52f = pml[grepl(\"^classe$|^roll_|^yaw_|^pitch_|^total_accel_|_x$|_y$|_z$\",names(pml))]\r\ntraining52f = pml52f[ inTrain,]\r\ntesting52f = pml52f[-inTrain,]\r\npml88f = pml52f %>% \r\n  mutate(gyros_belt_xy=gyros_belt_x*gyros_belt_y) %>%\r\n\tmutate(gyros_belt_xz=gyros_belt_x*gyros_belt_z) %>%\r\n\tmutate(gyros_belt_yz=gyros_belt_y*gyros_belt_z) %>%\r\n\tmutate(accel_belt_xy=accel_belt_x*accel_belt_y) %>%\r\n\tmutate(accel_belt_xz=accel_belt_x*accel_belt_z) %>%\r\n\tmutate(accel_belt_yz=accel_belt_y*accel_belt_z) %>%\r\n\tmutate(magnet_belt_xy=magnet_belt_x*magnet_belt_y) %>%\r\n\tmutate(magnet_belt_xz=magnet_belt_x*magnet_belt_z) %>%\r\n\tmutate(magnet_belt_yz=magnet_belt_y*magnet_belt_z) %>%\r\n\tmutate(gyros_arm_xy=gyros_arm_x*gyros_arm_y) %>%\r\n\tmutate(gyros_arm_xz=gyros_arm_x*gyros_arm_z) %>%\r\n\tmutate(gyros_arm_yz=gyros_arm_y*gyros_arm_z) %>%\r\n\tmutate(accel_arm_xy=accel_arm_x*accel_arm_y) %>%\r\n\tmutate(accel_arm_xz=accel_arm_x*accel_arm_z) %>%\r\n\tmutate(accel_arm_yz=accel_arm_y*accel_arm_z) %>%\r\n\tmutate(magnet_arm_xy=magnet_arm_x*magnet_arm_y) %>%\r\n\tmutate(magnet_arm_xz=magnet_arm_x*magnet_arm_z) %>%\r\n\tmutate(magnet_arm_yz=magnet_arm_y*magnet_arm_z) %>%\r\n\tmutate(gyros_dumbbell_xy=gyros_dumbbell_x*gyros_dumbbell_y) %>%\r\n\tmutate(gyros_dumbbell_xz=gyros_dumbbell_x*gyros_dumbbell_z) %>%\r\n\tmutate(gyros_dumbbell_yz=gyros_dumbbell_y*gyros_dumbbell_z) %>%\r\n\tmutate(accel_dumbbell_xy=accel_dumbbell_x*accel_dumbbell_y) %>%\r\n\tmutate(accel_dumbbell_xz=accel_dumbbell_x*accel_dumbbell_z) %>%\r\n\tmutate(accel_dumbbell_yz=accel_dumbbell_y*accel_dumbbell_z) %>%\r\n\tmutate(magnet_dumbbell_xy=magnet_dumbbell_x*magnet_dumbbell_y) %>%\r\n\tmutate(magnet_dumbbell_xz=magnet_dumbbell_x*magnet_dumbbell_z) %>%\r\n\tmutate(magnet_dumbbell_yz=magnet_dumbbell_y*magnet_dumbbell_z) %>%\r\n\tmutate(gyros_forearm_xy=gyros_forearm_x*gyros_forearm_y) %>%\r\n\tmutate(gyros_forearm_xz=gyros_forearm_x*gyros_forearm_z) %>%\r\n\tmutate(gyros_forearm_yz=gyros_forearm_y*gyros_forearm_z) %>%\r\n\tmutate(accel_forearm_xy=accel_forearm_x*accel_forearm_y) %>%\r\n\tmutate(accel_forearm_xz=accel_forearm_x*accel_forearm_z) %>%\r\n\tmutate(accel_forearm_yz=accel_forearm_y*accel_forearm_z) %>%\r\n\tmutate(magnet_forearm_xy=magnet_forearm_x*magnet_forearm_y) %>%\r\n\tmutate(magnet_forearm_xz=magnet_forearm_x*magnet_forearm_z) %>%\r\n\tmutate(magnet_forearm_yz=magnet_forearm_y*magnet_forearm_z) \r\ntraining88f = pml88f[ inTrain,]\r\ntesting88f = pml88f[-inTrain,]\r\nrfMod16f = train(classe~.,data=training16f,method=\"rf\",\r\n                 trControl=trainControl(method=\"cv\"))\r\nrfPred16f = predict(rfMod16f,testing16f)\r\nrfMod52f = train(classe~.,data=training52f,method=\"rf\",\r\n                 trControl=trainControl(method=\"cv\"))\r\nrfPred52f = predict(rfMod52f,testing52f)\r\nrfMod88f = train(classe~.,data=training88f,method=\"rf\",\r\n                 trControl=trainControl(method=\"cv\"))\r\nrfPred88f = predict(rfMod88f,testing88f)\r\nnnMod16f = multinom(classe~.,data=training16f)\r\nnnPred16f<-predict(nnMod16f,testing16f)\r\nnnMod52f = multinom(classe~.,data=training52f,maxit=250)\r\nnnPred52f<-predict(nnMod52f,testing52f)\r\nnnMod88f = multinom(classe~.,data=training88f,maxit=500)\r\nnnPred88f<-predict(nnMod88f,testing88f)\r\nlogModA16f<-glm(classe~.,mutate(training16f,classe=ifelse(classe==\"A\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictA16f<-predict(logModA16f,testing16f,type=\"response\")\r\nlogModB16f<-glm(classe~.,mutate(training16f,classe=ifelse(classe==\"B\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictB16f<-predict(logModB16f,testing16f,type=\"response\")\r\nlogModC16f<-glm(classe~.,mutate(training16f,classe=ifelse(classe==\"C\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictC16f<-predict(logModC16f,testing16f,type=\"response\")\r\nlogModD16f<-glm(classe~.,mutate(training16f,classe=ifelse(classe==\"D\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictD16f<-predict(logModD16f,testing16f,type=\"response\")\r\nlogModE16f<-glm(classe~.,mutate(training16f,classe=ifelse(classe==\"E\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictE16f<-predict(logModE16f,testing16f,type=\"response\")\r\nlogPredRaw16f<-data.frame(predictA16f,predictB16f,predictC16f,predictD16f,predictE16f)\r\nlogPred16f<-factor(apply(logPredRaw16f,1,which.max),labels=c(\"A\",\"B\",\"C\",\"D\",\"E\"))\r\nlogModA52f<-glm(classe~.,mutate(training52f,classe=ifelse(classe==\"A\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictA52f<-predict(logModA52f,testing52f,type=\"response\")\r\nlogModB52f<-glm(classe~.,mutate(training52f,classe=ifelse(classe==\"B\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictB52f<-predict(logModB52f,testing52f,type=\"response\")\r\nlogModC52f<-glm(classe~.,mutate(training52f,classe=ifelse(classe==\"C\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictC52f<-predict(logModC52f,testing52f,type=\"response\")\r\nlogModD52f<-glm(classe~.,mutate(training52f,classe=ifelse(classe==\"D\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictD52f<-predict(logModD52f,testing52f,type=\"response\")\r\nlogModE52f<-glm(classe~.,mutate(training52f,classe=ifelse(classe==\"E\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictE52f<-predict(logModE52f,testing52f,type=\"response\")\r\nlogPredRaw52f<-data.frame(predictA52f,predictB52f,predictC52f,predictD52f,predictE52f)\r\nlogPred52f<-factor(apply(logPredRaw52f,1,which.max),labels=c(\"A\",\"B\",\"C\",\"D\",\"E\"))\r\nlogModA88f<-glm(classe~.,mutate(training88f,classe=ifelse(classe==\"A\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictA88f<-predict(logModA88f,testing88f,type=\"response\")\r\nlogModB88f<-glm(classe~.,mutate(training88f,classe=ifelse(classe==\"B\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictB88f<-predict(logModB88f,testing88f,type=\"response\")\r\nlogModC88f<-glm(classe~.,mutate(training88f,classe=ifelse(classe==\"C\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictC88f<-predict(logModC88f,testing88f,type=\"response\")\r\nlogModD88f<-glm(classe~.,mutate(training88f,classe=ifelse(classe==\"D\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictD88f<-predict(logModD88f,testing88f,type=\"response\")\r\nlogModE88f<-glm(classe~.,mutate(training88f,classe=ifelse(classe==\"E\",TRUE,FALSE)),\r\n                family=\"binomial\",maxit=100)\r\npredictE88f<-predict(logModE88f,testing88f,type=\"response\")\r\nlogPredRaw88f<-data.frame(predictA88f,predictB88f,predictC88f,predictD88f,predictE88f)\r\nlogPred88f<-factor(apply(logPredRaw88f,1,which.max),labels=c(\"A\",\"B\",\"C\",\"D\",\"E\"))\r\nrfConfMat16f<-confusionMatrix(rfPred16f,testing16f$classe)\r\nrfAccuracy16f<-rfConfMat16f[[3]][1]\r\nrfConfMat52f<-confusionMatrix(rfPred52f,testing52f$classe)\r\nrfAccuracy52f<-rfConfMat52f[[3]][1]\r\nrfConfMat88f<-confusionMatrix(rfPred88f,testing88f$classe)\r\nrfAccuracy88f<-rfConfMat88f[[3]][1]\r\nnnConfMat16f<-confusionMatrix(nnPred16f,testing16f$classe)\r\nnnAccuracy16f<-nnConfMat16f[[3]][1]\r\nnnConfMat52f<-confusionMatrix(nnPred52f,testing52f$classe)\r\nnnAccuracy52f<-nnConfMat52f[[3]][1]\r\nnnConfMat88f<-confusionMatrix(nnPred88f,testing88f$classe)\r\nnnAccuracy88f<-nnConfMat88f[[3]][1]\r\nlogConfMat16f<-confusionMatrix(logPred16f,testing16f$classe)\r\nlogAccuracy16f<-logConfMat16f[[3]][1]\r\nlogConfMat52f<-confusionMatrix(logPred52f,testing52f$classe)\r\nlogAccuracy52f<-logConfMat52f[[3]][1]\r\nlogConfMat88f<-confusionMatrix(logPred88f,testing88f$classe)\r\nlogAccuracy88f<-logConfMat88f[[3]][1]\r\naccuracies<-data.frame(RandomForest=c(rfAccuracy16f,rfAccuracy52f,rfAccuracy88f),\r\n                       Multinom=c(nnAccuracy16f,nnAccuracy52f,nnAccuracy88f),\r\n                       LogisticRegression=c(logAccuracy16f,logAccuracy52f,logAccuracy88f),\r\n                       row.names=c(\"16Predictors\",\"52Predictors\",\"88Predictors\"))\r\n``````\r\nWeight Lifting Activity Recognition\r\n===================================\r\n\r\nIntroduction\r\n------------\r\nThe goal of this project is to obtain an algorithm that can evaluate the manner in which a person has performed a dumbbell curl, whether correctly or falling into one of 4 common mistakes, using data from accelerometers on the belt, forearm, arm, and on the dumbbell itself. The source of the data is http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises and it was presented in:  \r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.  \r\n(a link to the document is available at the end of the indicated page)\r\n\r\nDataset exploration and analysis\r\n--------------------------------\r\nThe dataset contains the following types of information:\r\n\r\n* Participant identifier (there were 6 different participants)\r\n* Timestamps for beginning and end of each observation\r\n* Statistical analysis data (observations grouped into windows, each with a window identifier and summary information that describes the distribution of values over that window)\r\n* Sensor data These are all the variables that start with \"pitch_\", \"roll_\", \"total_accel_\", \"yaw_\" (spherical magnitudes) or end with \"_x\", \"_y\" or \"_z\" (cartesian magtnitudes). \r\n* Identifier for how the exercise was performed:  \r\n  A-Exactly according to the specification  \r\n  B-Throwing the elbows to the front  \r\n  C-lifting the dumbbell only halfway  \r\n  D-lowering the dumbbell only halfway  \r\n  E-throwing the hips to the front  \r\n\r\nEach observation corresponds to one 'rep' (a repetition of the curl exercise). \r\n\r\nThe sensor data will be used as predictors. \r\n\r\n3 different sets of data were analyzed:  \r\n\r\n* Spherical magnitudes only. Having both Tait-Bryan angles and magnitude (spherical coordinates) and 3-Dimensional euclidean space x,y,z (cartesian magnitudes} is actually two ways of expressing the same information, and the information is redundant. Using just one set reduces the amount of data in the model, making it faster to train. The Spherical coordinates are easier to interpret intuitively and that is why I have chosen this set. The resulting dataset uses 16 predictors. Note: Another possibility for obtaining a smaller dataset would have been to apply principal component analysis but the results are likely less intuitive, that is why I didn't choose them.\r\n* Full spherical and cartesian coordinates. Even though the information is redundant, keeping both spherical and cartesian coordinates can be an effective way to have covariates and solve underfit problems. The resulting dataset has 52 predictors.\r\n* Cross-product cartesian features. One possibility for additional covariates are the cartesian xy, xz and yz cross-products. For models where underfit is a problem these can improve the resulting accuracy. The resulting dataset has 88 predictors\r\n\r\nThe Identifier for how the exercise was performed, classe, will be the outcome of the algorithm and it needs to be included in the datasets as well. \r\n\r\nWe want to get partitions that are varied and not skewed towards specific outcomes or participants so we will use classe and user_name for partitioning. We will use 60% of the data for training and 40% for testing. Testing data will not be used for refining the model so it is not necessary to further add a validation set. Cross-validation and Out of sample error estimation will be performed on the testing set.\r\n\r\nThe participant identifier is not used as a predictor in order to obtain a general algorithm that is not specific to an individual participant. The statistical analysis data must not be used as a predictor as it is specific to this dataset and not generalizable to future data.\r\n\r\nTo get an idea of what the data looks like, we can see the plot for the Tait-Bryan angles and magnitude features\r\n```{r featurePlot, fig.width=10, fig.height=10}\r\nplt<-featurePlot(x=pml[grepl(\"^roll_|^yaw_|^pitch_|^total_accel_\",names(pml))],\r\n                 y=pml$classe,plot=\"boxplot\",layout=c(4,4))\r\nprint(plt)\r\n```\r\n\r\nAlgorithm and results\r\n---------------------\r\nWe are looking for a categorical result therefore we need a classifier. The outcome is available therefore supervised learning techniques will be more effective. Several modeling techniques were explored: Logistic regression, neural networks and Random forests. In terms of computational requirements, random forests are the slowest, requiring a very significant amount of training time as the number of features grows. Neural networks were the fastest to train but the accuracy with these feature sets was lower than the other methods. Accuracy improved with the addition of features, suggesting that it may be possible to improve the model by adding new features (e.g., we could add squared cartesian magnitudes). Logistic regression was slightly slower than neural networks but they provided perfect accuracy even on the small feature set. Training the logistic regression on the larger feature set took less time than the random forests on the smaller set. We use cross-validation as a training option.\r\n\r\nThe percentual Out of sample error estimates obtained from cross-validation on the different models and feature sets are the following:\r\n\r\n```{r} \r\nround(100*(1-accuracies),2)\r\n```\r\n\r\nBased on these results our recommendation is to apply Random Forest using the medium feature set (rfMod52f), or possibly the small feature set (rfMod16f) if training performance is a concern. Their confusion matrices are:  \r\n```{r} \r\nrfConfMat16f\r\nrfconfMat52f\r\n```\r\n\r\nApplying these 2 algorithms to the provided Test Data Set result in:\r\n```{r}\r\nTestSet<-read.csv2(\"pml-testing.csv\", header=TRUE, sep = \",\", dec=\".\", na.strings=\"NA\")\r\ndata.frame(rfMod16f=predict(rfMod16f,TestSet),rfMod52f=predict(rfMod52f,TestSet))\r\n```\r\nBoth sets of predicted outcomes are identical.\r\n\r\nAppendix: Reference Code\r\n------------------------\r\n```{r ref.label = \"Code\", eval = FALSE, echo = TRUE}\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}